# -*- coding: utf-8 -*-
"""Copy of Copy of CV - how to unrar in colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KnSjl-AoiChZVv6dzkrfRrDZdc4BMenO

###Step 1: Download the zip file
"""

!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip

"""### Step 2: Import OS and make a folder "dataset"
"""

import os

!mkdir dataset

"""### Step 3: Unzip and unrar images into the folder "dataset"
"""

!unzip /content/5y9wdsg2zt-2.zip

!unrar x /content/"Concrete Crack Images for Classification.rar" /content/dataset

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers, losses, callbacks, applications
import numpy as np
import matplotlib.pyplot as plt
import os, datetime
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
from tensorflow.keras.utils import plot_model

file_path = r'/content/dataset'

BATCH_SIZE = 100
IMG_SIZE = (224,224)

file_dataset = keras.utils.image_dataset_from_directory(file_path,batch_size=BATCH_SIZE,image_size=IMG_SIZE,shuffle=True)

# take first batch of test data as the test dataset, the rest will be validation dataset
val_dataset = file_dataset.skip(1)
test_dataset = file_dataset.take(1)

#3. Convert the datasets into PrefetchDataset
AUTOTUNE = tf.data.AUTOTUNE
pf_train = file_dataset.prefetch(buffer_size=AUTOTUNE)
pf_val = val_dataset.prefetch(buffer_size=AUTOTUNE)
pf_test = test_dataset.prefetch(buffer_size=AUTOTUNE)

#4. Create the data augmentation model
data_augmentation = keras.Sequential()
data_augmentation.add(layers.RandomFlip('horizontal'))
data_augmentation.add(layers.RandomRotation(0.2))

#5. Create the input preprocessing layer
preprocess_input = applications.mobilenet_v2.preprocess_input

#6. Apply transfer learning
class_names = file_dataset.class_names
nClass = len(class_names)
#(A) Apply transfer learning to create the feature extractor
IMG_SHAPE = IMG_SIZE + (3,)
base_model = applications.MobileNetV3Large(input_shape=IMG_SHAPE,include_top=False,weights="imagenet",include_preprocessing=False)
base_model.trainable = False

#(B) Create the classifier
global_avg = layers.GlobalAveragePooling2D()
output_layer = layers.Dense(nClass,activation='softmax')

#7. Link the layers together to form the model pipeline using functional API
inputs = keras.Input(shape=IMG_SHAPE)
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x,training=False)
x = global_avg(x)
# x = layers.Dropout(0.3)(x)
outputs = output_layer(x)

model = keras.Model(inputs=inputs,outputs=outputs)
model.summary()
plot_model(model, show_shapes=True)

#8. Compile the model
cos_decay = optimizers.schedules.CosineDecay(0.0005,50)
optimizer = optimizers.Adam(learning_rate=cos_decay)
loss = losses.SparseCategoricalCrossentropy()
model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])

#9. Evaluate the model before training
loss0,acc0 = model.evaluate(pf_test)
print("----------------Evaluation Before Training-------------------")
print("Loss = ",loss0)
print("Accuracy = ",acc0)

#10. Create tensorboard

log_dir = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))

tb_callback = TensorBoard(log_dir=log_dir)

#early stopping

es_callback = EarlyStopping(monitor='loss',patience=5)

hist = model.fit(pf_train,validation_data=pf_val,epochs=5,callbacks=[tb_callback])

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

#12. Follow-up training
base_model.trainable = True
for layer in base_model.layers[:200]:
    layer.trainable = False
base_model.summary()

#13. Compile the model
optimizer = optimizers.RMSprop(learning_rate=0.00001)
model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])

#14. Continue training the model
fine_tune_epoch = 5
total_epoch = 5 + fine_tune_epoch
history_fine = model.fit(pf_train,validation_data=pf_val,epochs=total_epoch,initial_epoch = hist.epoch[-1],callbacks=[tb_callback])

#15. Evaluate the model after training
test_loss, test_acc = model.evaluate(pf_test)
print("----------------Evaluation After Training---------------")
print("Test loss = ",test_loss)
print("Test accuracy = ",test_acc)

#16. Model deployment
image_batch, label_batch = pf_test.as_numpy_iterator().next()
y_pred = np.argmax(model.predict(image_batch),axis=1)
#Stack the label and prediction in one numpy array
label_vs_prediction = np.transpose(np.vstack((label_batch,y_pred)))

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

save_path = os.path.join("save_model", "model.h5")
model.save(save_path)